# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18435BTzjn8iTtX7wNDnBoZztDagYvGnY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve, f1_score, precision_score, recall_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Example Dataset (replace with real fraud dataset)
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=10, n_informative=4, weights=[0.95], flip_y=0.01, random_state=42)
feature_names = [f"feature_{i}" for i in range(X.shape[1])]

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Models
rf = RandomForestClassifier(random_state=42)
lr = LogisticRegression(max_iter=1000)

rf.fit(X_train, y_train)
lr.fit(X_train, y_train)

# Predictions
models = {'Random Forest': rf, 'Logistic Regression': lr}
predictions = {name: model.predict(X_test) for name, model in models.items()}
probas = {name: model.predict_proba(X_test)[:, 1] for name, model in models.items()}

# 1. Confusion Matrices
for name, y_pred in predictions.items():
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()
    plt.title(f'Confusion Matrix - {name}')
    plt.show()
    # Shows true positives, false positives, etc. Helpful to assess fraud detection performance.

# 2. ROC Curve
plt.figure(figsize=(8, 6))
for name, y_score in probas.items():
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()
# ROC Curve: The higher the curve, the better the model at distinguishing fraud vs. non-fraud.

# 3. Precision-Recall Curve
plt.figure(figsize=(8, 6))
for name, y_score in probas.items():
    precision, recall, _ = precision_recall_curve(y_test, y_score)
    plt.plot(recall, precision, label=name)
plt.title('Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.show()
# Precision-Recall: Especially helpful for imbalanced data like fraud detection.

# 4. Feature Importance (Random Forest)
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 6))
sns.barplot(x=importances[indices], y=np.array(feature_names)[indices])
plt.title("Feature Importance - Random Forest")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()
# Highlights top fraud indicators, e.g., transaction amount or time anomalies.

# 5. Model Comparison Table
metrics = {'Model': [], 'F1-Score': [], 'Precision': [], 'Recall': []}
for name, y_pred in predictions.items():
    metrics['Model'].append(name)
    metrics['F1-Score'].append(f1_score(y_test, y_pred))
    metrics['Precision'].append(precision_score(y_test, y_pred))
    metrics['Recall'].append(recall_score(y_test, y_pred))

metrics_df = pd.DataFrame(metrics)
metrics_df.set_index('Model').plot(kind='bar', figsize=(10, 6))
plt.title("Model Comparison: F1-Score, Precision, Recall")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()